{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbcf6d9e",
   "metadata": {},
   "source": [
    "# Training a Two-Layer Neural Network on Cifar-10 --\n",
    "\n",
    "The tendril classification problem allowed us to use Neural Networks on a 2D toy dataset. In this notebook, we will work with an $n$-dimensional dataset of images, where $n$ is the total size (# pixels x # color-channels) of an image. We will be using the famed [cifar-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html), so that our  model can classify pictures of cars, planes, cats, dogs, frogs, and other items. There are 10 classes in total represented in this dataset. Each image has is an 32 pixels by 32 pixels RGB image of shape ``(3,32,32)``. Thus each image is a point or vector in $\\mathbb{R}^{3072}$.\n",
    "\n",
    "We will be training a two-layer neural network. Our loss function is the cross-entropy loss. The first two layers will use the ReLU activation function and the last layer will use softmax activation.\n",
    "\n",
    "\n",
    "#### The Model in Full\n",
    "\n",
    "\\begin{equation}\n",
    "D_1(x) = \\operatorname{ReLU}(xW_{1} + b_{1})\\\\\n",
    "D_2(x) = \\operatorname{ReLU}(D_1(x) W_{2} + b_{3})\\\\\n",
    "F(\\{W\\}, \\{b\\}; x) = \\operatorname{softmax}(D_2(x) W_3+b_3)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We will again be using the popular cross-entropy classification loss. Keep in mind that `mygrad`, and other auto-differentiation libraries, provide a convenient softmax_crossentropy function, which efficiently computes the softmax *and then* the corss-entropy. So take care to not invoke softmax twice, in  following the equations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef61bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mygrad as mg\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2277c6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "datasets.download_cifar10()\n",
    "x_train, y_train, x_test, y_test = datasets.load_cifar10()\n",
    "\n",
    "print('Training data shape: ', x_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', x_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print(x_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358cbee0",
   "metadata": {},
   "source": [
    "Let's investigate what our data roughly looks like. Plotting some sample images from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a413d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(x_train[idx].transpose(1,2,0).astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98da885e",
   "metadata": {},
   "source": [
    "Flatten out x_train and x_test and use ``astype`` to convert your data to ``np.float32``. Your ``(3,32,32)`` image should now be ``(3072,)``. Additionally, find the mean image and standard deviation image of the training and test data. Then, zero-center your data by subtracting the mean image and normalize by dividing out by the standard deviation image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc4df0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the train and test data to be shape (N, 3072)  (N=50,000 for train, N=10,000 for test)\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "# Using only the training data, compute the mean-image (a shape-(3072,) array)\n",
    "# and the std-dev image (also shape-(3072,) array)\n",
    "#\n",
    "# That is, we are computing the mean 1st pixel over all images, the mean 2nd pixel,\n",
    "# and so on, creating a shape-(3072,) array of these mean pixel values.\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "# Using the *same* mean and std, do a pixel-wise normalization of the testing\n",
    "# data: x_test = (x_test - mean) / std\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768407e1",
   "metadata": {},
   "source": [
    "Now, let's construct our model using `MyNN` and define our [accuracy function](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/Problems/ComputeAccuracy.html).\n",
    "\n",
    "We can experiment with the sizes of our layers, but try:\n",
    "\n",
    "- layer-1: size-100\n",
    "- layer-2: size-50\n",
    "- layer-3: size-? (hint: we don't get to pick this)\n",
    "  - This final layer should not have any bias: `dense(..., bias=False)`\n",
    "  - We will be using `softmax_crossentropy` for our loss, we don't need an activation function for this last layer.\n",
    "\n",
    "Use the `he_normal` initialization for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6009eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mygrad.nnet.initializers.he_normal import he_normal\n",
    "from mygrad.nnet.activations.relu import relu\n",
    "from mygrad.nnet.losses import softmax_crossentropy\n",
    "\n",
    "from mynn.optimizers.sgd import SGD\n",
    "from mynn.layers.dense import dense\n",
    "\n",
    "\n",
    "# Define your MyNN-`Model` class here. It should have:\n",
    "# - an `__init__` method that initializes all of your layers\n",
    "# - a `__call__` method that defines the model's \"forward pass\"\n",
    "# - a `parameters` property that returns a tuple of all of your\n",
    "#   model's learnable parameters (refer to the Tendrils-MyNN)\n",
    "#   notebook for the syntax of defining a class-property)\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, n1, n2, num_classes):\n",
    "        \"\"\"\n",
    "        Initializes a model with two hidden layers of size `n1` and `n2`\n",
    "        respectively.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n1 : int\n",
    "            The number of neurons in the first hidden layer\n",
    "\n",
    "        n2 : int\n",
    "            The number of neurons in the second hidden layer\n",
    "\n",
    "        num_classes : int\n",
    "            The number of classes predicted by the model\"\"\"\n",
    "        # STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecab5313",
   "metadata": {},
   "source": [
    "Initialize your model and optimizer, using SGD from MyNN. Specify the parameters, learning rate and weight_decay for your\n",
    "optimizer.\n",
    "\n",
    "A learning rate of $0.1$ and a weight decay of $5\\times10^{-4}$ is sensible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ce943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82816436",
   "metadata": {},
   "source": [
    "Now write code to train your model! Experiment with your learning rate and weight_decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2c28a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `batch_size = 100`: the number of predictions that we will make in each training step\n",
    "\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "# We will train for 10 epochs; you can change this if you'd like.\n",
    "# You will likely want to train for much longer than this\n",
    "for epoch_cnt in range(10):\n",
    "\n",
    "    # Create the indices to index into each image of your training data\n",
    "    # e.g. `array([0, 1, ..., 9999])`, and then shuffle those indices.\n",
    "    # We will use this to draw random batches of data\n",
    "    # STUDENT CODE HERE\n",
    "\n",
    "    for batch_cnt in range(0, len(x_train) // batch_size):\n",
    "        # Index into `x_train` to get your batch of M images.\n",
    "        # Make sure that this is a randomly-sampled batch\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "        # compute the predictions for this batch by calling on model\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "\n",
    "        # compute the true (a.k.a desired) values for this batch:\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "\n",
    "        # compute the loss associated with our predictions(use softmax_cross_entropy)\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "\n",
    "        # back-propagate through your computational graph through your loss\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "\n",
    "        # execute gradient-descent by calling step() of optim\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "\n",
    "        # compute the accuracy between the prediction and the truth\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "\n",
    "        plotter.set_train_batch({\"loss\" : loss.item(),\n",
    "                                 \"accuracy\" : acc},\n",
    "                                 batch_size=batch_size)\n",
    "\n",
    "    # After each epoch we will evaluate how well our model is performing\n",
    "    # on data from cifar10 *that it has never \"seen\" before*. This is our\n",
    "    # \"test\" data. The measured accuracy of our model here is our best\n",
    "    # estimate for how our model will perform in the real world\n",
    "    # (on 32x32 RGB images of things in this class)\n",
    "    test_idxs = np.arange(len(x_test))\n",
    "\n",
    "    for batch_cnt in range(0, len(x_test)//batch_size):\n",
    "        batch_indices = test_idxs[batch_cnt*batch_size : (batch_cnt + 1)*batch_size]\n",
    "\n",
    "        batch = x_test[batch_indices]\n",
    "        truth = y_test[batch_indices]\n",
    "\n",
    "        # We do not want to compute gradients here, so we use the\n",
    "        # no_autodiff context manager to disable the ability to\n",
    "        with mg.no_autodiff:\n",
    "            # Get your model's predictions for this test-batch\n",
    "            # and measure the test-accuracy for this test-batch\n",
    "            # STUDENT CODE HERE\n",
    "\n",
    "        # pass your test-accuracy here; we used the name `test_accuracy`\n",
    "        plotter.set_test_batch({\"accuracy\" : test_accuracy}, batch_size=batch_size)\n",
    "    plotter.set_test_epoch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6985a38b",
   "metadata": {},
   "source": [
    "## Evaluating Your Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc73a6d",
   "metadata": {},
   "source": [
    "How well is your model performing? Is there any discrepancy between how well it does on training data vs testing data?\n",
    "\n",
    "Below, we provide code to randomly pick an image from the test set, plot it, and print your model's predicted label vs the true label. `datasets.load_cifar10.labels` returns a tuple of the label-names in correspondence with each truth-index.\n",
    "\n",
    "Since we shifted and normalized our data, we have to re-load the data here, using different names for the arrays.\n",
    "\n",
    "Note that we still need to pass your model the shifted/normalized test images. So the data you use to plot the image is different from the data that you pass to the model. Also note that your model expects a *batch* of images, not a single image. Thus we use a batch of size-1, which has shape-(1, 3072) - your model will produce a shape-(1, 10) tensor of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851bfeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, img_test, label_test = datasets.load_cifar10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45076516",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = datasets.load_cifar10.labels  # tuple of cifar-10 labels\n",
    "\n",
    "index = np.random.randint(0, len(img_test))  # pick a random test-image index\n",
    "\n",
    "true_label_index = label_test[index]\n",
    "true_label = labels[true_label_index]\n",
    "\n",
    "with mg.no_autodiff:\n",
    "    prediction = model(x_test[index:index + 1])  # you must pass in a shape-(1, 3072) array\n",
    "    predicted_label_index = np.argmax(prediction.data, axis=1).item()  # largest score indicates the prediction\n",
    "    predicted_label = labels[predicted_label_index]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# matplotlib wants shape-(H, W, C) images, with unsigned 8bit pixel values\n",
    "img = img_test[index].transpose(1,2,0).astype('uint8')\n",
    "\n",
    "ax.imshow(img)\n",
    "ax.set_title(f\"Predicted: {predicted_label}\\nTruth: {true_label}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ef954",
   "metadata": {},
   "source": [
    "Can you understand some of the mistakes that your model is making? Perhaps it sees a white plane over water, and confuses it for a boat. Can *you* figure out what some of these images depict? Some are pretty hard to identify, given the low resolution."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "nbsphinx"
  },
  "kernelspec": {
   "display_name": "week2",
   "language": "python",
   "name": "week2"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
